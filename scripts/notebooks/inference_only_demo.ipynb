{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CA2ztRmbST8x"
      },
      "source": [
        "# InternVLA-N1 Model Inferring Notebook\n",
        "\n",
        "This notebook is used to infer the InternVLA-N1 model's by reading images and instructions from local folders. If you'd like to test the model in real world or self-built dataset, you could follow this tutorial without large **datasets download** and **simulation environment setup** (isaac-sim or habitat). Let's start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPsA_xNST8x"
      },
      "source": [
        "## 0. Preparation\n",
        "### 0.0 Create Conda Environment\n",
        "First, we should create a conda environment through `conda create -n internvla python=3.9` and launch the jupyter kernel using the created environment. In the following tutorial, we assume the environment name is `internvla`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWE4cXG4ST8y",
        "outputId": "b2abd82d-e324-45b0-f9c2-b684d103bbd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Collecting torch==2.6.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m827.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.2.0 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.6.0) (75.2.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.6.0)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision==0.21.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.6.0) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp312-cp312-linux_x86_64.whl (768.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m768.4/768.4 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp312-cp312-linux_x86_64.whl (7.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.2.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n"
          ]
        }
      ],
      "source": [
        "%pip install torch==2.6.0 torchvision==0.21.0 --index-url https://download.pytorch.org/whl/cu124\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIyAIqwYST8y"
      },
      "source": [
        "We recommend to install flash-attn2 via pre-built wheel. If you have trouble with the installation, you might also skip this installation and remove the line of `attn_implementation=\"flash_attention_2\"` in the model initialization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VIv1FC0PST8y"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl\n",
        "%pip install flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp39-cp39-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdOQ6ZNfST8z"
      },
      "outputs": [],
      "source": [
        "%pip install transformers==4.51.0 diffusers==0.31.0 accelerate==1.10.1 opencv-python==4.10.0.82 pillow==10.4.0 numpy==1.26.4 gym==0.23.1\n",
        "%pip install imageio==2.37.0 imageio-ffmpeg==0.6.0 ftfy==6.3.1\n",
        "%pip install scipy matplotlib\n",
        "%pip install -e ../../. # install InternNav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_hO-fWlST8z"
      },
      "source": [
        "### 0.1 Prepare the dataset for inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QJdMx1kST8z"
      },
      "outputs": [],
      "source": [
        "!tar -xvf ../../assets/realworld_sample_data.tar.gz -C ../../assets/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJfxOEJjST8z"
      },
      "source": [
        "### 0.2 Download checkpoint\n",
        "The size of checkpoint is about 8GB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3po1JX-ST80"
      },
      "outputs": [],
      "source": [
        "!mkdir -p checkpoints && cd checkpoints && git clone https://huggingface.co/InternRobotics/InternVLA-N1\n",
        "!git lfs pull"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M-HumldST80"
      },
      "source": [
        "### 0.3 Download the DepthAnything checkpoint\n",
        "Download the depthanything checkpoint from [DepthAnything](https://huggingface.co/depth-anything/Depth-Anything-V2-Small/blob/14cf9f3d82acd6b6c9b43fa50b79a639a4e69c8d/depth_anything_v2_vits.pth) and move it into `scripts/eval/checkpoints/checkpoints` folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCP9bMWzST80"
      },
      "source": [
        "## 1. Import Required Libraries\n",
        "Because we haven't prepared environment for other baselineas, such as CMA and RDP, we need to remove sereral lines in `internnav/agent/__init__.py`.\n",
        "```bash\n",
        "from internnav.agent.base import Agent\n",
        "# from internnav.agent.cma_agent import CmaAgent\n",
        "# from internnav.agent.rdp_agent import RdpAgent\n",
        "# from internnav.agent.seq2seq_agent import Seq2SeqAgent\n",
        "from internnav.agent.internvla_n1_agent import InternVLAN1Agent\n",
        "```\n",
        "\n",
        "If you meet the error about the `No module named LongCLIP (or diffusion policy)`, you should run the `git submodule update --init` in the root directory of InternNav."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKQmwQPtST80"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# Add project path\n",
        "project_root = Path('../../')\n",
        "sys.path.insert(0, str(project_root))\n",
        "sys.path.insert(0, str(project_root / 'src/diffusion-policy'))\n",
        "\n",
        "from internnav.agent.internvla_n1_agent_realworld import InternVLAN1AsyncAgent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9wxWVUfST80"
      },
      "source": [
        "## 2. Configure Parameters\n",
        "Change the checkpoint path of InternVLA-N1 to the exact path in your computer. In real-world experiment, too fast inference will lead to overly close the memory intervals of the model, resulting in a large sim-to-real gap. Therefore, we use an argument `plan_step_gap` to make the model only infer every `plan_step_gap` frames when outputing trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sSeccBXST80"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "    def __init__(self):\n",
        "        self.device = \"cuda:0\"\n",
        "        self.model_path = \"checkpoints/InternVLA-N1\"\n",
        "        self.resize_w = 384\n",
        "        self.resize_h = 384\n",
        "        self.num_history = 8\n",
        "        self.camera_intrinsic = np.array([\n",
        "            [386.5, 0.0, 328.9, 0.0],\n",
        "            [0.0, 386.5, 244.0, 0.0],\n",
        "            [0.0, 0.0, 1.0, 0.0],\n",
        "            [0.0, 0.0, 0.0, 1.0]\n",
        "        ])\n",
        "        self.plan_step_gap = 8\n",
        "\n",
        "args = Args()\n",
        "print(f\"Model path: {args.model_path}\")\n",
        "print(f\"Device: {args.device}\")\n",
        "print(f\"Image size: {args.resize_w}x{args.resize_h}\")\n",
        "print(f\"History frames: {args.num_history}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-3tceI9ST80"
      },
      "source": [
        "## 3. Initialize Agent\n",
        "Load the InternVLA-N1 model and initialize the agent. If you meet error about transformers, please check that the `flash_attn` and `accelerate` is correctly installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5LVBwbqST80"
      },
      "outputs": [],
      "source": [
        "print(\"Loading model...\")\n",
        "agent = InternVLAN1AsyncAgent(args)\n",
        "\n",
        "# Warm up model\n",
        "print(\"Warming up model...\")\n",
        "dummy_rgb = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "dummy_depth = np.zeros((480, 640), dtype=np.float32)\n",
        "dummy_pose = np.eye(4)\n",
        "agent.reset()\n",
        "agent.step(dummy_rgb, dummy_depth, dummy_pose, \"hello\", intrinsic=args.camera_intrinsic)\n",
        "print(\"Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_OlYYT5ST80"
      },
      "source": [
        "## 4. Configure Test Data Path\n",
        "Now we used a pre-collected real-world dataset to test our model. The images were captured through a Unitree Go2 robot mounted with a realsense D455. You could freely change the dataset to your own dataset and an `instruction.txt` file. Note that the `InternVLA-N1` model use depth image inputs for S1 model, but we forgot recording the depth image in real-world dataset. If you want to build your own dataset, please record **both the aligned depth and rgb images**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ue3JVhGOST80"
      },
      "outputs": [],
      "source": [
        "# Configure data directory (single scene per folder)\n",
        "scene_dir = '../../assets/realworld_sample_data1'\n",
        "\n",
        "# Check if instruction file exists\n",
        "instruction_path = os.path.join(scene_dir, 'instruction.txt')\n",
        "if not os.path.exists(instruction_path):\n",
        "    print(f\"Error: instruction.txt not found in {scene_dir}\")\n",
        "else:\n",
        "    print(f\"Scene directory: {scene_dir}\")\n",
        "\n",
        "    # Read instruction\n",
        "    with open(instruction_path, 'r') as f:\n",
        "        instruction = f.read().strip()\n",
        "    print(f\"Instruction: {instruction}\")\n",
        "\n",
        "    # Get all debug_raw images\n",
        "    rgb_paths = sorted(glob.glob(os.path.join(scene_dir, 'debug_raw_*.jpg')))\n",
        "    print(f\"\\nFound {len(rgb_paths)} images\")\n",
        "    # Show first few image names\n",
        "    print(\"\\nFirst 5 images:\")\n",
        "    for i, path in enumerate(rgb_paths[:5]):\n",
        "        print(f\"  {i+1}. {os.path.basename(path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8bANT_RST81"
      },
      "source": [
        "Now add some visualization function for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms75xbTaST81"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import cv2\n",
        "\n",
        "def annotate_image(idx, image, llm_output, trajectory, pixel_goal, output_dir):\n",
        "    image = Image.fromarray(image)#.save(f'rgb_{idx}.png')\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    font_size = 20\n",
        "    font = ImageFont.truetype(\"DejaVuSansMono.ttf\", font_size)\n",
        "    text_content = []\n",
        "    text_content.append(f\"Frame    Id  : {idx}\")\n",
        "    text_content.append(f\"Actions      : {llm_output}\" )\n",
        "    max_width = 0\n",
        "    total_height = 0\n",
        "    for line in text_content:\n",
        "        bbox = draw.textbbox((0, 0), line, font=font)\n",
        "        text_width = bbox[2] - bbox[0]\n",
        "        text_height = 26\n",
        "        max_width = max(max_width, text_width)\n",
        "        total_height += text_height\n",
        "\n",
        "    padding = 10\n",
        "    box_x, box_y = 10, 10\n",
        "    box_width = max_width + 2 * padding\n",
        "    box_height = total_height + 2 * padding\n",
        "\n",
        "    draw.rectangle([box_x, box_y, box_x + box_width, box_y + box_height], fill='black')\n",
        "\n",
        "    text_color = 'white'\n",
        "    y_position = box_y + padding\n",
        "\n",
        "    for line in text_content:\n",
        "        draw.text((box_x + padding, y_position), line, fill=text_color, font=font)\n",
        "        bbox = draw.textbbox((0, 0), line, font=font)\n",
        "        text_height = 26\n",
        "        y_position += text_height\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Draw trajectory visualization in the top-right corner using matplotlib\n",
        "    if trajectory is not None and len(trajectory) > 0:\n",
        "        import matplotlib.pyplot as plt\n",
        "        from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
        "\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # Window parameters\n",
        "        window_size = 200  # Window size in pixels\n",
        "        window_margin = 0  # Margin from edge\n",
        "        window_x = img_width - window_size - window_margin\n",
        "        window_y = window_margin\n",
        "\n",
        "        # Extract trajectory points\n",
        "        traj_points = []\n",
        "        for point in trajectory:\n",
        "            if isinstance(point, (list, tuple, np.ndarray)) and len(point) >= 2:\n",
        "                traj_points.append([float(point[0]), float(point[1])])\n",
        "\n",
        "        if len(traj_points) > 0:\n",
        "            traj_array = np.array(traj_points)\n",
        "            x_coords = traj_array[:, 0]\n",
        "            y_coords = traj_array[:, 1]\n",
        "\n",
        "            # Create matplotlib figure\n",
        "            fig, ax = plt.subplots(figsize=(2, 2), dpi=100)\n",
        "            fig.patch.set_alpha(0.6)  # Semi-transparent background\n",
        "            fig.patch.set_facecolor('gray')\n",
        "            ax.set_facecolor('lightgray')\n",
        "\n",
        "            # Plot trajectory\n",
        "            # Coordinate system: x-axis points up, y-axis points left\n",
        "            # Origin at bottom center\n",
        "            ax.plot(y_coords, x_coords, 'b-', linewidth=2, label='Trajectory')\n",
        "\n",
        "            # Mark start point (green) and end point (red)\n",
        "            ax.plot(y_coords[0], x_coords[0], 'go', markersize=6, label='Start')\n",
        "            ax.plot(y_coords[-1], x_coords[-1], 'ro', markersize=6, label='End')\n",
        "\n",
        "            # Mark origin\n",
        "            ax.plot(0, 0, 'w+', markersize=10, markeredgewidth=2, label='Origin')\n",
        "\n",
        "            # Set axis labels\n",
        "            ax.set_xlabel('Y (left +)', fontsize=8)\n",
        "            ax.set_ylabel('X (up +)', fontsize=8)\n",
        "            ax.invert_xaxis()\n",
        "            ax.tick_params(labelsize=6)\n",
        "            ax.grid(True, alpha=0.3, linewidth=0.5)\n",
        "\n",
        "            # Set equal aspect ratio\n",
        "            ax.set_aspect('equal', adjustable='box')\n",
        "\n",
        "            # Add legend\n",
        "            ax.legend(fontsize=6, loc='upper right')\n",
        "\n",
        "            # Adjust layout\n",
        "            plt.tight_layout(pad=0.3)\n",
        "\n",
        "            # Convert matplotlib figure to numpy array\n",
        "            canvas = FigureCanvasAgg(fig)\n",
        "            canvas.draw()\n",
        "            plot_img = np.frombuffer(canvas.tostring_rgb(), dtype=np.uint8)\n",
        "            plot_img = plot_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "            plt.close(fig)\n",
        "\n",
        "            # Resize plot to fit window\n",
        "            plot_img = cv2.resize(plot_img, (window_size, window_size))\n",
        "\n",
        "            # Overlay plot on image\n",
        "            image[window_y:window_y+window_size, window_x:window_x+window_size] = plot_img\n",
        "\n",
        "    if pixel_goal is not None:\n",
        "        cv2.circle(image, (pixel_goal[1], pixel_goal[0]), 5, (255, 0, 0), -1)\n",
        "    image = Image.fromarray(image).convert('RGB')\n",
        "    image.save(f'{output_dir}/rgb_{idx}_annotated.png')\n",
        "    # to numpy array\n",
        "    return np.array(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Ixb17zST81"
      },
      "source": [
        "## 5. Run Model Testing\n",
        "We begin to read the local images, instruction to run the model. Please make sure that the depth image is fed to the model and **the unit is in meter**. You could print the maximum value of the depth image in your real-world experiment and check the value.\n",
        "\n",
        "If everything goes well, the model will rotate in place at the begining. Then it generates the correct pixel goal and trajectories. The visualization results are also saved in the `save_dir` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4HzYflVST81"
      },
      "outputs": [],
      "source": [
        "# Reset agent\n",
        "agent.reset()\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Processing scene: {os.path.basename(scene_dir)}\")\n",
        "print(f\"Instruction: '{instruction}'\")\n",
        "print(f\"Total images: {len(rgb_paths)}\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "\n",
        "action_seq = []\n",
        "look_down = False\n",
        "\n",
        "save_dir = '../../test_data/'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "# Process each image\n",
        "for i, rgb_path in enumerate(rgb_paths):\n",
        "    # Check if this is a look_down image\n",
        "    look_down = ('look_down' in rgb_path)\n",
        "\n",
        "    # Extract image ID from filename (e.g., debug_raw_0003.jpg -> 0003)\n",
        "    basename = os.path.basename(rgb_path)\n",
        "    if look_down:\n",
        "        # e.g., debug_raw_0010_look_down.jpg -> 0010\n",
        "        image_id = basename.replace('debug_raw_', '').replace('_look_down.jpg', '')\n",
        "    else:\n",
        "        # e.g., debug_raw_0003.jpg -> 0003\n",
        "        image_id = basename.replace('debug_raw_', '').replace('.jpg', '')\n",
        "\n",
        "    # Read RGB image\n",
        "    rgb = np.asarray(Image.open(rgb_path).convert('RGB'))\n",
        "\n",
        "    # Create dummy depth image (not available in test data)\n",
        "    # !Note You must full in depth to model\n",
        "    depth = np.zeros((rgb.shape[0], rgb.shape[1]), dtype=np.float32)\n",
        "\n",
        "    # Create dummy camera pose\n",
        "    camera_pose = np.array([\n",
        "        [1, 0, 0, 0],\n",
        "        [0, 1, 0, 0],\n",
        "        [0, 0, 1, 0],\n",
        "        [0, 0, 0, 1]\n",
        "    ])\n",
        "\n",
        "    # Run model or just save image\n",
        "    # print(f\"[{i+1}/{len(rgb_paths)}] Running model inference: {os.path.basename(rgb_path)}\")\n",
        "    dual_sys_output = agent.step(\n",
        "        rgb,\n",
        "        depth,\n",
        "        camera_pose,\n",
        "        instruction,\n",
        "        intrinsic=args.camera_intrinsic,\n",
        "        look_down=look_down\n",
        "    )\n",
        "\n",
        "    # Print output results\n",
        "    if dual_sys_output.output_action is not None and dual_sys_output.output_action != []:\n",
        "        print(f\"  Output action: {dual_sys_output.output_action}\")\n",
        "        # action_seq.extend(s2_output.output_action)\n",
        "    else:\n",
        "\n",
        "        print(f\"output_trajectory: {dual_sys_output.output_trajectory.tolist()}\")\n",
        "        if dual_sys_output.output_pixel is not None:\n",
        "            print(f\"output_pixel: {dual_sys_output.output_pixel}\")\n",
        "            annotate_image(image_id, rgb, 'traj', dual_sys_output.output_trajectory.tolist(), dual_sys_output.output_pixel, save_dir)\n",
        "\n",
        "\n",
        "print(f\"\\nScene {os.path.basename(scene_dir)} completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_NdoCQQST81"
      },
      "source": [
        "# 6.Visualize Results\n",
        "It's worth noting that we input an zero depth image to the model, so the output trajectories are short. In your own experiments, please check the output lengths of the model are about **2m**. If not, you should check the model inputs or create au issue on Github."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTV1KdnUST81"
      },
      "outputs": [],
      "source": [
        "\n",
        "import glob\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for img_path in sorted(glob.glob(f'{save_dir}/*_annotated.png')):\n",
        "    plt.imshow(Image.open(img_path))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "internvla",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.24"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}